{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## We will give tour of most prominent decoding models, mainly Greedy search, Beam search, Top-K sampling and Top P"
      ],
      "metadata": {
        "id": "xTdG8VzBNq5n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfWUh_HmNpfz",
        "outputId": "206a066d-937a-4317-b33c-05360ee7229c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.1 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q tensorflow==2.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Add Tokenizer\n",
        "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Add EOS token as PAD to avoid warnings\n",
        "model=TFGPT2LMHeadModel.from_pretrained('gpt2',pad_token_id=tokenizer.eos_token_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrcGOngyOmyY",
        "outputId": "816ebbaa-461c-43cf-dafc-d5c52d743766"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy Search-\n",
        "* Greedy search is a simple text generation method where at each time step, the model selects the highest probability token as the next word. It does not look ahead"
      ],
      "metadata": {
        "id": "QvVbajkJQUal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the context\n",
        "input_ids=tokenizer.encode('I enjoy walking with my cute dog',return_tensors='tf')\n",
        "\n",
        "# Generate the output\n",
        "greedy_output=model.generate(input_ids,max_length=50)\n",
        "\n",
        "print(tokenizer.decode(greedy_output[0],skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6RczWPlOXLp",
        "outputId": "141e1cc4-ac28-4962-e1ad-7a885f08a9b7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
            "\n",
            "I'm not sure if I'll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The biggest drawbask of greedy search,is it misses the high probability words hidden behind low probability words. So it starts repeting the words.\n"
      ],
      "metadata": {
        "id": "WB90x8_RTlb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Search -\n",
        "Beam search reduce the risk of missing hidden high probability word sequence by keeping most likely `num_beams` of hypotheses at each time step and eventually choosing the hypothesis that has the overall high probability."
      ],
      "metadata": {
        "id": "t48XGUObUKoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_output=model.generate(input_ids,max_length=50,num_beams=5,early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(beam_output[0],skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6jGId2cSXLa",
        "outputId": "10f44b90-9065-4d52-ccbc-de5221a56457"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
            "\n",
            "I'm not sure if I'll ever be able to walk with him again. I'm not sure if I'll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So as we can see our model repeat the sentence again and again. So we are going to use n-gram penalties.N-gram penalties are teh mechanism to prevent repetitive text generation by penalizing the model if it tries to generate the same same N-gram more than once.\n"
      ],
      "metadata": {
        "id": "csGeLAR_WVvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_generate=model.generate(input_ids,max_length=50,num_beams=5,no_repeat_ngram_size=2,early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output_generate[0],skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbRKu5cYVXN-",
        "outputId": "653955a2-d6a4-4fa1-bf90-aacb22143b35"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
            "\n",
            "I've been thinking about this for a while now, and I think it's time for me to take a break\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have `num_return_sequences` which will help us to determines how many of the best beams are returned at the end. It is used with combination of `num_beams`- which determines how many search paths are explored during text generation.\n"
      ],
      "metadata": {
        "id": "KS5IkWBpZTK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_return_sequences=5,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# now we have 3 output sequences\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84QYXb_zXa6b",
        "outputId": "18d56c0b-967c-4278-c0c6-4812e256409c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
            "\n",
            "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
            "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
            "\n",
            "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
            "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
            "\n",
            "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
            "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
            "\n",
            "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
            "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
            "\n",
            "I've been thinking about this for a while now, and I think it's time for me to take a step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the problem over is that as we seen that it generates the output according to the high probability of the token  so as human being we do not expect the output like that , which will generate on probabilities, we want output like we do not expect the common output . so we use the sampling method where we select randomly next token with conditional probabilities"
      ],
      "metadata": {
        "id": "vsMA9-8OeKys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling-\n",
        "Sampling means randomly picking next token according to conditional probability distribution."
      ],
      "metadata": {
        "id": "XyVrIEPOe9CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "sample_output=model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_k=0\n",
        "    )\n",
        "print(tokenizer.decode(sample_output[0],skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjtzn9tcac0K",
        "outputId": "54f72489-63e4-4ffe-f4fc-cadcb99b014d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I enjoy walking with my cute dog and never seem to get nervous until it turns into battery Art Hours $50 of Weekend Bikes Friday Night A King's Approaches This was all, no problem WRONG!! WOWMS readers: excellent product!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So there is risk of incoherent text. Model might generate sentence that do not make sense grammatically\n",
        "* To generate output sharper we use the `temperature`"
      ],
      "metadata": {
        "id": "2_ijq95Kf_2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output=model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    top_k=0,\n",
        "    temperature=0.7\n",
        ")\n",
        "print(tokenizer.decode(sample_output[0],skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsBOS8Aafl5m",
        "outputId": "ffc1392e-d76e-4ed8-fcf5-e38677f582f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I enjoy walking with my cute dog. He's just a little quiet and quiet. He's so hungry. He loves puppies, so we're all so so excited to have him. We'll be loving him all the time.\"\n",
            "\n",
            "Dr.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top-K sampling-\n",
        "Model chooses the next token from top `k` most probable tokens. The model first calculate the probability of all tokens. Only the top `k` tokens with the highest probabilites are kept.\n",
        "* This introduce creativity and diversity because the next token is chosen randomly.\n",
        "* It avoids the predictible and repetative words as we can see in greedy search .\n"
      ],
      "metadata": {
        "id": "UV6ZsTfLiGMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output=model.generate(\n",
        "          input_ids,\n",
        "          max_length=50,\n",
        "          do_sample=True,\n",
        "          top_k=50\n",
        "\n",
        ")\n",
        "print(tokenizer.decode(sample_output[0],skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lorjyJsQgn3z",
        "outputId": "c275d437-2f1c-465b-b9de-752d74433fd8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I enjoy walking with my cute dog. He won't touch me like that! He's a little timid. He makes me feel very small in my home. Maybe it's possible for me not to do this with a puppy but he is a nice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So there is some problem in the Top-K sampling\n",
        "* Top-k always considers a fixed number of most probable tokens.\n",
        "* This is static, which means it does not adapt the context of the text.\n"
      ],
      "metadata": {
        "id": "-J7bDMYSkxEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Top-p (nucleus) sampling**\n",
        "\n",
        "Instead of sampling only from the most likely *K* words, in *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*."
      ],
      "metadata": {
        "id": "o6uLDjLBlyIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRKdO0ZGlD0F",
        "outputId": "3cec9129-d7e2-42d8-98f4-bca91b6f59eb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I enjoy walking with my cute dog and very often see her immediately excited at moving on.\" - Alicia Davis\n",
            "\n",
            "Recently, running around Los Angeles on an expeditions had me thinking, \"What'd I get for free here?\" I worked as a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Tg4AxAl0mp",
        "outputId": "59d96d79-fdf5-4108-c52f-65512eb6d33f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I enjoy walking with my cute dog\n",
            "\n",
            "In my life I love having fun\n",
            "\n",
            "What does it cost to feed a cat to get a heart attack?\n",
            "\n",
            "I think, this is actually a really good question: what are some of the\n",
            "1: I enjoy walking with my cute dog as much as I do driving my car. It's the best way to spend a Saturday without missing a beat.\n",
            "\n",
            "5/5 - My favorite weekend in my life. The weekend I really wanted is Tuesday\n",
            "2: I enjoy walking with my cute dog, I like to get a dog from the grocery store and it's been a pleasure to sit and talk to her (when she is sleeping), and she's been so happy with our visit to the supermarket (we\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kfwvk1qYl2te"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}