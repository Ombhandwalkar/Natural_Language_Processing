{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ks1Q_cX1RqPg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers[torch]\n",
        "!pip install rouge_score  evaluate datasets\n",
        "! pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer,T5ForConditionalGeneration,Seq2SeqTrainer,Seq2SeqTrainingArguments,DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YSBmG3U6Sags"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset('multi_news')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BudLUt-LTeJS",
        "outputId": "192caf52-ffa3-4734-ba5e-c2e2cc0804e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkAfC4WVTxhx",
        "outputId": "0f6519d4-9d16-4bd6-c3ae-319e75bc7840"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['document', 'summary'],\n",
              "        num_rows: 44972\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['document', 'summary'],\n",
              "        num_rows: 5622\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['document', 'summary'],\n",
              "        num_rows: 5622\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'].features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxCtdrHET2kR",
        "outputId": "a71fdfc4-3d16-4442-897d-d600c5c9e1f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'document': Value(dtype='string', id=None),\n",
              " 'summary': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the subset of dataset for finetuning purpose\n",
        "train_subset=dataset['train'].select(range(10000))\n",
        "validation_subset=dataset['validation'].select(range(1000))\n",
        "test_subset=dataset['test'].select(range(1000))"
      ],
      "metadata": {
        "id": "g8Jjw6KgT69P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializng the tokenizer\n",
        "models='t5-small'\n",
        "tokenizer=T5Tokenizer.from_pretrained(models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPBj9GQ4UQ5C",
        "outputId": "89afd111-c30c-4393-b3d0-dd316ea08643"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "  inputs=['summarize'+  doc for doc in examples['document']]\n",
        "  model_inputs=tokenizer(inputs,max_length=512,truncation=True)\n",
        "\n",
        "  # Setup tokenizer for target\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels=tokenizer(examples['summary'],max_length=512,truncation=True)\n",
        "\n",
        "  model_inputs['labels']=labels['input_ids']\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "zXk9lreqUfWN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize dataset\n",
        "tokenized_train=train_subset.map(preprocess_function,batched=True)\n",
        "tokenized_validation=validation_subset.map(preprocess_function,batched=True)\n",
        "tokenized_test=validation_subset.map(preprocess_function,batched=True)"
      ],
      "metadata": {
        "id": "nNHOkQHNXVLt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model=T5ForConditionalGeneration.from_pretrained(models)"
      ],
      "metadata": {
        "id": "fCNUS8kMXuAE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collator\n",
        "data_collator=DataCollatorForSeq2Seq(tokenizer,model=model)"
      ],
      "metadata": {
        "id": "vS_hp9XgYRja"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROUGE Metrics\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of summaries by comparing them to reference summaries (typically human-generated). ROUGE is particularly popular in the field of natural language processing for tasks such as summarization. The metrics focus on different aspects of the generated summary and provide insights into its quality. The main ROUGE metrics include:\n",
        "\n",
        "## ROUGE-N\n",
        "Measures the overlap of n-grams between the candidate summary and the reference summary. The most common versions are ROUGE-1 (unigrams) and ROUGE-2 (bigrams).\n",
        "\n",
        "### ROUGE-1\n",
        "Counts the overlap of single words.\n",
        "- **ROUGE-1 Recall**:\n",
        "  $$\n",
        "  \\text{ROUGE-1 Recall} = \\frac{\\text{Number of overlapping unigrams}}{\\text{Total unigrams in reference summary}}\n",
        "  $$\n",
        "- **ROUGE-1 Precision**:\n",
        "  $$\n",
        "  \\text{ROUGE-1 Precision} = \\frac{\\text{Number of overlapping unigrams}}{\\text{Total unigrams in candidate summary}}\n",
        "  $$\n",
        "- **ROUGE-1 F1-Score**:\n",
        "  $$\n",
        "  \\text{ROUGE-1 F1-Score} = 2 \\times \\frac{\\text{ROUGE-1 Recall} \\times \\text{ROUGE-1 Precision}}{\\text{ROUGE-1 Recall} + \\text{ROUGE-1 Precision}}\n",
        "  $$\n",
        "\n",
        "**Example Calculation for ROUGE-1:**\n",
        "\n",
        "Given a reference summary \"The cat sat on the mat.\" and a candidate summary \"The cat is on the mat.\", calculate ROUGE-1:\n",
        "- Unigrams in Reference: {The, cat, sat, on, the, mat}\n",
        "- Unigrams in Candidate: {The, cat, is, on, the, mat}\n",
        "- Overlap: {The, cat, on, the, mat}\n",
        "- Recall: $ \\frac{5}{6} $\n",
        "- Precision: $ \\frac{5}{6} $\n",
        "- F1-Score: $ 2 \\times \\frac{5/6 \\times 5/6}{5/6 + 5/6} = 0.833 $\n",
        "\n",
        "### ROUGE-2\n",
        "Counts the overlap of two-word sequences.\n",
        "- **ROUGE-2 Recall**:\n",
        "  $$\n",
        "  \\text{ROUGE-2 Recall} = \\frac{\\text{Number of overlapping bigrams}}{\\text{Total bigrams in reference summary}}\n",
        "  $$\n",
        "- **ROUGE-2 Precision**:\n",
        "  $$\n",
        "  \\text{ROUGE-2 Precision} = \\frac{\\text{Number of overlapping bigrams}}{\\text{Total bigrams in candidate summary}}\n",
        "  $$\n",
        "- **ROUGE-2 F1-Score**:\n",
        "  $$\n",
        "  \\text{ROUGE-2 F1-Score} = 2 \\times \\frac{\\text{ROUGE-2 Recall} \\times \\text{ROUGE-2 Precision}}{\\text{ROUGE-2 Recall} + \\text{ROUGE-2 Precision}}\n",
        "  $$\n",
        "\n",
        "**Example Calculation for ROUGE-2:**\n",
        "\n",
        "Using the same reference and candidate summaries:\n",
        "- Bigrams in Reference: {The cat, cat sat, sat on, on the, the mat}\n",
        "- Bigrams in Candidate: {The cat, cat is, is on, on the, the mat}\n",
        "- Overlap: {The cat, on the, the mat}\n",
        "- Recall: $ \\frac{3}{5} = 0.600 $\n",
        "- Precision: $ \\frac{3}{5} = 0.600 $\n",
        "- F1-Score: $ 2 \\times \\frac{0.6 \\times 0.6}{0.6 + 0.6} = 0.600 $\n",
        "\n",
        "## ROUGE-L\n",
        "Measures the longest common subsequence (LCS) between the candidate and reference summaries. This captures the longest sequence of words that appear in both summaries in the same order, reflecting the importance of sentence-level structure.\n",
        "- **ROUGE-L Recall**:\n",
        "  $$\n",
        "  \\text{ROUGE-L Recall} = \\frac{\\text{LCS}}{\\text{Total words in reference summary}}\n",
        "  $$\n",
        "- **ROUGE-L Precision**:\n",
        "  $$\n",
        "  \\text{ROUGE-L Precision} = \\frac{\\text{LCS}}{\\text{Total words in candidate summary}}\n",
        "  $$\n",
        "- **ROUGE-L F1-Score**:\n",
        "  $$\n",
        "  \\text{ROUGE-L F1-Score} = 2 \\times \\frac{\\text{ROUGE-L Recall} \\times \\text{ROUGE-L Precision}}{\\text{ROUGE-L Recall} + \\text{ROUGE-L Precision}}\n",
        "  $$\n",
        "\n",
        "**Example Calculation for ROUGE-L:**\n",
        "\n",
        "Using the same reference and candidate summaries:\n",
        "- LCS: \"The cat on the mat\"\n",
        "- Recall: $ \\frac{5}{6} \\approx 0.833 $\n",
        "- Precision: $ \\frac{5}{6} \\approx 0.833 $\n",
        "- F1-Score: $ 2 \\times \\frac{0.833 \\times 0.833}{0.833 + 0.833} = 0.833 $\n"
      ],
      "metadata": {
        "id": "ZpC46pVzebr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define compute_metrics function\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    # Decode the predictions and labels\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, use_stemmer=True)\n",
        "\n",
        "    # Aggregate the ROUGE scores\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in pred_ids]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "JKMFkEu1YbHh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seq2Seq training arguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",             # Directory to save model checkpoints and logs\n",
        "    learning_rate=2e-5,                 # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=16,     # Batch size for training\n",
        "    per_device_eval_batch_size=16,      # Batch size for evaluation\n",
        "    weight_decay=0.01,                  # Weight decay for regularization\n",
        "    save_total_limit=3,                 # Limit the total number of checkpoints saved\n",
        "    num_train_epochs=3,                 # Number of training epochs\n",
        "    predict_with_generate=True,         # Use generation mode for prediction\n",
        "    generation_max_length=150,          # Maximum length for generated sequences\n",
        "    generation_num_beams=4,             # Number of beams for beam search during generation\n",
        ")"
      ],
      "metadata": {
        "id": "-m59-AU3g4nq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                       # The model to be trained\n",
        "    args=training_args,                # Training arguments defined with Seq2SeqTrainingArguments\n",
        "    train_dataset=tokenized_train,     # The training dataset\n",
        "    eval_dataset=tokenized_validation, # The evaluation dataset\n",
        "    data_collator=data_collator,       # The data collator for processing data batches\n",
        "    tokenizer=tokenizer,               # The tokenizer used for preprocessing\n",
        "    compute_metrics=compute_metrics,   # The function to compute evaluation metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukBUmAQYhD4H",
        "outputId": "13644cae-0b01-49ac-e1b1-9573b10b86fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-b6f1b6fc2f98>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "45Qkq99whZRR",
        "outputId": "1f119c3e-fef1-4dcc-82a3-8f55dff070cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mombhandwalkar38126\u001b[0m (\u001b[33mombhandwalkar38126-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250429_075319-kazohi9u</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ombhandwalkar38126-student/huggingface/runs/kazohi9u' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/ombhandwalkar38126-student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ombhandwalkar38126-student/huggingface' target=\"_blank\">https://wandb.ai/ombhandwalkar38126-student/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ombhandwalkar38126-student/huggingface/runs/kazohi9u' target=\"_blank\">https://wandb.ai/ombhandwalkar38126-student/huggingface/runs/kazohi9u</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on validation set\n",
        "trainer.evaluate()\n",
        "\n",
        "# Evaluate the model on test set\n",
        "test_results = trainer.evaluate(eval_dataset=tokenized_test)\n",
        "\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "uYJZ4U30hbvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Select a specific data point from the test dataset\n",
        "test_index = 0  # Change this index to the specific data point you want to summarize\n",
        "example_text = dataset[\"test\"][test_index][\"document\"]\n",
        "\n",
        "# Preprocess the input text\n",
        "input_text = \"summarize: \" + example_text\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = inputs.to(device)\n",
        "# Generate the summary\n",
        "summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode the generated summary\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Original Text:\\n\", example_text)\n",
        "print(\"\\nGenerated Summary:\\n\", summary)"
      ],
      "metadata": {
        "id": "huPnF680h2kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xLodk7e0h4aT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}