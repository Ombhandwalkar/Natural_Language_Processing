{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzDwM_4qtyP8",
        "outputId": "1c055693-8eef-4902-eb04-d90b07d99136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-18 11:18:25--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
            "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 18.155.68.60, 18.155.68.58, 18.155.68.74, ...\n",
            "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|18.155.68.60|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 312733741 (298M) [text/plain]\n",
            "Saving to: â€˜oscar.eo.txtâ€™\n",
            "\n",
            "oscar.eo.txt        100%[===================>] 298.25M  45.8MB/s    in 4.6s    \n",
            "\n",
            "2025-05-18 11:18:30 (65.3 MB/s) - â€˜oscar.eo.txtâ€™ saved [312733741/312733741]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# in this notebook we'll only get one of the files (the Oscar one) for the sake of simplicity and performance\n",
        "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Tokenizer"
      ],
      "metadata": {
        "id": "tOkcqHaavQDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We won't need TensorFlow here\n",
        "!pip uninstall -y tensorflow\n",
        "# Install `transformers` from master\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n",
        "# transformers version at notebook update --- 2.11.0\n",
        "# tokenizers version at notebook update --- 0.8.0rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_12ZVwbulR4",
        "outputId": "1fbe1e92-ed93-4998-f52c-c9ea56b62871"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-lk_lufu1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-lk_lufu1\n",
            "  Resolved https://github.com/huggingface/transformers to commit 40a493c7ed4f19f08eadb0639cf26d49bfa5e180\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2025.4.26)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.52.0.dev0-py3-none-any.whl size=11327449 sha256=5112d1498a1c1f5e6790ee1af92a462046c9fec12aa0a1d6dac7f8979ae328a9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hlbl9hmu/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed transformers-4.52.0.dev0\n",
            "sentence-transformers                 4.1.0\n",
            "tokenizers                            0.21.1\n",
            "transformers                          4.52.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths=[str(x)for x in Path('.').glob('**/*.txt')]\n",
        "\n",
        "# Initialize the tokenzer\n",
        "tokenizer=ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize the training\n",
        "tokenizer.train(files=paths,vocab_size=52_000,min_frequency=2,special_tokens=['<s>','<pad>','</s>','<unk>','<mask>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP_YFd5xvW_R",
        "outputId": "a9e0e25a-4ac2-4f8c-acee-8e06d2ebcc4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15min 52s, sys: 5.74 s, total: 15min 58s\n",
            "Wall time: 9min 43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the file to disk\n",
        "!mkdir EsperBERTo\n",
        "tokenizer.save_model('EsperBERTo')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hif2xoTiwvce",
        "outputId": "d577144e-618b-4acd-9d7f-6183cc681642"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EsperBERTo/vocab.json', 'EsperBERTo/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer=ByteLevelBPETokenizer(\n",
        "    \"./EsperBERTo/vocab.json\",\n",
        "    \"./EsperBERTo/merges.txt\",)"
      ],
      "metadata": {
        "id": "2gnQ-PlxyN_K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer._tokenizer.post_processor=BertProcessing(\n",
        "    ('</s>',tokenizer.token_to_id('</s>')),\n",
        "    ('<s>',tokenizer.token_to_id('<s>'))\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "metadata": {
        "id": "EYVMLKOlzlMc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"Mi estas Julien.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4LMy22F0mgg",
        "outputId": "c78d1f97-dfff-4f1b-977f-146b6d46ea6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"Mi estas Julien.\").tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbOQt3Cc0qF1",
        "outputId": "d7052254-f3e0-48ae-b975-6154793b4906"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'Mi', 'Ä estas', 'Ä Juli', 'en', '.', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that we have a GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkcOPatz0yuI",
        "outputId": "2bdb4595-e241-47c7-bad6-e2f8db963470"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 18 11:29:21 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will define model config\n",
        "from transformers import RobertaConfig\n",
        "\n",
        "config=RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=12,\n",
        "    type_vocab_size=1\n",
        ")"
      ],
      "metadata": {
        "id": "v_40GVrn1uCn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer=RobertaTokenizerFast.from_pretrained('./EsperBERTo',max_len=512)"
      ],
      "metadata": {
        "id": "Q77zXytT19KD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "model=RobertaForMaskedLM(config=config)"
      ],
      "metadata": {
        "id": "G1du-Spk2mj6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.num_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXBJN71J3-xo",
        "outputId": "cb90990e-9533-4bb1-9b72-cc6fc99331d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83502880"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset=LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='./oscar.eo.txt',\n",
        "    block_size=128\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcgvT3iz4BLg",
        "outputId": "bd7a8f25-7ff2-44a9-c2ce-8e2f546db610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=True,mlm_probability=0.15)\n"
      ],
      "metadata": {
        "id": "k7fmEeIL408L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our trainer\n",
        "from transformers import Trainer,TrainingArguments\n",
        "\n",
        "training_args=TrainingArguments(\n",
        "    output_dir='./EsperBERTo',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True\n",
        ")\n",
        "trainer=Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")"
      ],
      "metadata": {
        "id": "rnpidfT-6kel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "XgyEE5MN7PWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gy37AASt7SYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}