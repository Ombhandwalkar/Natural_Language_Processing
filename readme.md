[**Transformers-Scratch**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/Transformers_from_scratch)
Transformers a deep learning architecture intruduced in the paper **"Attention Is All You Need"** 
---
[**LLaMa-2-Scratch**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/LLaMA-2-7b_from_scratch)
Recreated the core components of LLaMA-2 (By META) transformer model from scratch usin Pytorch.
---

[**PEFT**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/PEFT)
Using Hugging Face's **PEFT** implemented 
**IA3**, **LoRA**, **AdaLoRA**, **QLoRA**, **Prefix Tuning**, 
---

[**Nano-GPT**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/Nano_GPT)
Build a Generatively Pretrained Transformer (GPT), following the  OpenAI's GPT-2 
---

[**NLP**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/NLP)
Performed different types of operations.
---
[**Quantization**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/Quantization)
It is a technique to reduce memory and computational cost of models by representing weights and activations with lower precision.
---


[**Bert-Deploy**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/BERT_Deploy)
A BERT-based NLP model to classify whether a given comment is toxic or non-toxic.
---

[**HuggingFace**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/HuggingFace)
Hands-on experience on HuggingFace libraries.
---

[**LoRA**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/LoRA)
LoRA is parameter efficient fine tuning technique that lets you adapt large pretrained models like BERT with much low compute memory and storage.
--


[**vLLM**](https://github.com/Ombhandwalkar/Natural_Language_Processing/tree/master/vLLM)
Technique to minimize the computation and time of the LLM

---